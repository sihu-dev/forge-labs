# ğŸš€ HEPHAITOS Supabase Master Plan

**ì‘ì„±ì¼**: 2025-12-17
**ëª©í‘œ**: Dashboard Pro-level â†’ Backend Enterprise-level
**ê¸°ê°„**: 4 weeks (Beta Week 1-4)
**ì˜ˆìƒ íš¨ê³¼**: Performance 10x, Scalability 100x, UX Excellence

---

## ğŸ“Š Executive Summary

### Current State (As-Is)
- **Dashboard**: 100/100 âœ… Pro-level (Robinhood + TradingView + Linear)
- **Backend**: 60/100 âš ï¸ Basic Supabase usage
- **Infrastructure**: 50/100 âš ï¸ Scalability concerns
- **Overall**: 70/100

### Target State (To-Be)
- **Dashboard**: 100/100 âœ… Real-time enabled
- **Backend**: 95/100 âœ… Database Functions, Triggers, Cache
- **Infrastructure**: 95/100 âœ… Connection Pooling, Monitoring
- **Overall**: 97/100

### Gap Analysis
| Category | Gap | Impact | Priority |
|----------|-----|--------|----------|
| Real-time Updates | âŒ WebSocket ì—†ìŒ | ğŸ”´ Critical | P0 |
| N+1 Queries | âŒ API 300ms | ğŸ”´ Critical | P0 |
| Connection Pooling | âŒ Scalability ìœ„í—˜ | ğŸŸ¡ High | P0 |
| Cache Layer | âŒ DB ë¶€í•˜ | ğŸŸ¡ High | P1 |
| Database Triggers | âŒ ìˆ˜ë™ ì‘ì—… | ğŸŸ¢ Medium | P2 |
| Storage Integration | âŒ ê¸°ëŠ¥ ë¶€ì¡± | ğŸŸ¢ Low | P3 |
| Scheduled Tasks | âŒ ìš´ì˜ ë¹„íš¨ìœ¨ | ğŸŸ¢ Low | P3 |

---

## ğŸ¯ 7-Phase Implementation Plan

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        User Browser                          â”‚
â”‚                    (Next.js 15 Frontend)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Vercel Edge Network                       â”‚
â”‚         (Serverless Functions + Static Hosting)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Supabase Platform                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Realtime   â”‚  â”‚  PostgREST   â”‚  â”‚ Edge Functions  â”‚  â”‚
â”‚  â”‚  (WebSocket) â”‚  â”‚   (API)      â”‚  â”‚   (Webhooks)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                 â”‚                    â”‚            â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                           â†“                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚            PostgreSQL Database                        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚   Tables   â”‚  â”‚ Functions  â”‚  â”‚   Triggers    â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  + RLS     â”‚  â”‚ (Business) â”‚  â”‚ (Automation)  â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ Materializedâ”‚  â”‚  Indexes   â”‚  â”‚   pg_cron    â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   Views    â”‚  â”‚ (Perf opt) â”‚  â”‚ (Scheduled)  â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â†•                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚          Supavisor (Connection Pooler)               â”‚  â”‚
â”‚  â”‚     Transaction Mode: 500 max connections            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â†•                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚            Supabase Storage                           â”‚  â”‚
â”‚  â”‚  Buckets: backtest-charts, exports, templates        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Upstash Redis (Cache + Queue)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Cache Layer     â”‚         â”‚   BullMQ Queue       â”‚     â”‚
â”‚  â”‚ (Strategies,     â”‚         â”‚ (Backtest Jobs)      â”‚     â”‚
â”‚  â”‚  Leaderboard)    â”‚         â”‚                      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“… Timeline & Phases

### Week 1: Beta Launch + Critical Infrastructure (Dec 17-24)

#### **Phase 0: Pre-Beta Launch** âœ… (TODAY)
- [x] Pro-level Dashboard ë°°í¬ ì™„ë£Œ
- [x] Production í…ŒìŠ¤íŠ¸
- [ ] Beta ì´ˆëŒ€ ë°œì†¡ (ì²« 20ëª…)
- [ ] Discord ì„œë²„ ì˜¤í”ˆ

#### **Phase 3: Connection Pooling** ğŸ”¥ (Day 1, 1 hour)
**Why First**: Scalability í™•ë³´, Beta traffic spike ëŒ€ë¹„

**Checklist**:
- [ ] Supabase Dashboard â†’ Settings â†’ Database
- [ ] Enable Supavisor Connection Pooling
- [ ] Copy `DATABASE_POOLER_URL`
- [ ] Update `.env.local` + Vercel env vars
- [ ] Test connection: `psql $DATABASE_POOLER_URL`
- [ ] Deploy & monitor connection count

**Expected Outcome**:
- Max connections: 100 â†’ 500+ (pooled)
- Connection latency: -50ms
- Zero "too many connections" errors

---

#### **Phase 1: Real-time Dashboard** ğŸ”¥ (Day 2-3, 8-11 hours)
**Why Second**: User Experience ê·¹ì  ê°œì„ 

**Step 1: Enable Realtime (30ë¶„)**
```sql
-- Supabase Dashboard â†’ Database â†’ Replication
ALTER PUBLICATION supabase_realtime ADD TABLE strategies;
ALTER PUBLICATION supabase_realtime ADD TABLE backtest_jobs;
ALTER PUBLICATION supabase_realtime ADD TABLE backtest_results;

-- Verify
SELECT schemaname, tablename
FROM pg_publication_tables
WHERE pubname = 'supabase_realtime';
```

**Step 2: Create Realtime Hooks (2-3ì‹œê°„)**
```typescript
// hooks/useRealtimeStrategies.ts
import { useEffect, useState } from 'react';
import { supabase } from '@/lib/supabase';

export function useRealtimeStrategies(userId: string) {
  const [strategies, setStrategies] = useState<Strategy[]>([]);
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    // Initial fetch
    const fetchStrategies = async () => {
      const { data } = await supabase
        .from('strategies')
        .select('*')
        .eq('user_id', userId)
        .order('created_at', { ascending: false });

      setStrategies(data || []);
      setLoading(false);
    };

    fetchStrategies();

    // Subscribe to changes
    const channel = supabase
      .channel('strategies-realtime')
      .on('postgres_changes', {
        event: '*',
        schema: 'public',
        table: 'strategies',
        filter: `user_id=eq.${userId}`
      }, (payload) => {
        if (payload.eventType === 'INSERT') {
          setStrategies(prev => [payload.new as Strategy, ...prev]);
        } else if (payload.eventType === 'UPDATE') {
          setStrategies(prev => prev.map(s =>
            s.id === payload.new.id ? payload.new as Strategy : s
          ));
        } else if (payload.eventType === 'DELETE') {
          setStrategies(prev => prev.filter(s => s.id !== payload.old.id));
        }
      })
      .subscribe();

    return () => {
      supabase.removeChannel(channel);
    };
  }, [userId]);

  return { strategies, loading };
}

// Similarly create:
// - hooks/useRealtimeBacktestJobs.ts
// - hooks/useRealtimePerformance.ts
```

**Step 3: Dashboard Integration (2-3ì‹œê°„)**
```typescript
// app/dashboard/page.tsx
'use client';

import { useAuth } from '@/hooks/useAuth';
import { useRealtimeStrategies } from '@/hooks/useRealtimeStrategies';
import { ActiveStrategies } from '@/components/dashboard/ActiveStrategies';
import { PortfolioHero } from '@/components/dashboard/PortfolioHero';

export default function DashboardPage() {
  const { user } = useAuth();
  const { strategies, loading } = useRealtimeStrategies(user.id);

  if (loading) return <LoadingSpinner />;

  return (
    <div className="min-h-screen bg-[#0A0A0B]">
      <PortfolioHero totalValue={12345.67} change={567.89} changePercent={4.82} />

      {/* Real-time updating strategies */}
      <ActiveStrategies strategies={strategies} />
    </div>
  );
}
```

**Step 4: Toast Notifications (2ì‹œê°„)**
```typescript
// components/ToastProvider.tsx
'use client';

import { useEffect } from 'react';
import { toast } from 'sonner';
import { supabase } from '@/lib/supabase';
import { useAuth } from '@/hooks/useAuth';

export function ToastProvider({ children }: { children: React.ReactNode }) {
  const { user } = useAuth();

  useEffect(() => {
    if (!user) return;

    // Listen for backtest completions
    const channel = supabase
      .channel('user-notifications')
      .on('postgres_changes', {
        event: 'UPDATE',
        schema: 'public',
        table: 'backtest_jobs',
        filter: `user_id=eq.${user.id}`
      }, (payload) => {
        const job = payload.new;

        if (job.status === 'completed') {
          toast.success('Backtest Complete!', {
            description: `Strategy "${job.strategy_name}" finished.`
          });
        } else if (job.status === 'failed') {
          toast.error('Backtest Failed', {
            description: `Strategy "${job.strategy_name}" encountered an error.`
          });
        }
      })
      .subscribe();

    return () => {
      supabase.removeChannel(channel);
    };
  }, [user]);

  return <>{children}</>;
}
```

**Step 5: Testing & Debugging (1-2ì‹œê°„)**
- [ ] Open 2 browser tabs (same user)
- [ ] Tab 1: Create new strategy
- [ ] Tab 2: Verify it appears instantly
- [ ] Check Network tab: WebSocket connection active
- [ ] Test reconnection: Disable WiFi â†’ Enable â†’ Should reconnect

**Step 6: Performance Optimization (1ì‹œê°„)**
```typescript
// hooks/useDebounce.ts (prevent too many renders)
export function useDebounce<T>(value: T, delay: number): T {
  const [debouncedValue, setDebouncedValue] = useState<T>(value);

  useEffect(() => {
    const timer = setTimeout(() => setDebouncedValue(value), delay);
    return () => clearTimeout(timer);
  }, [value, delay]);

  return debouncedValue;
}

// Usage in realtime hook
const debouncedStrategies = useDebounce(strategies, 100); // 100ms
```

**Expected Outcome**:
- Dashboard updates in real-time (no refresh)
- Backtest progress shown live
- Toast notifications on events
- WebSocket connection stable (< 5% reconnection rate)

---

### Week 2: Performance Optimization (Dec 24-31)

#### **Phase 5: Cache Layer** (Day 1-2, 16 hours)
**Why Before Functions**: Immediate performance boost

**Implementation**:
```typescript
// lib/cache.ts
import { Redis } from '@upstash/redis';

const redis = new Redis({
  url: process.env.UPSTASH_REDIS_REST_URL!,
  token: process.env.UPSTASH_REDIS_REST_TOKEN!
});

interface CacheOptions {
  ttl?: number; // seconds
  tags?: string[];
}

export async function cached<T>(
  key: string,
  fetcher: () => Promise<T>,
  options: CacheOptions = {}
): Promise<T> {
  const { ttl = 300, tags = [] } = options;

  // Try cache
  const cached = await redis.get<T>(key);
  if (cached !== null) return cached;

  // Fetch from source
  const data = await fetcher();

  // Store in cache
  await redis.set(key, data, { ex: ttl });

  // Store tags for invalidation
  for (const tag of tags) {
    await redis.sadd(`tag:${tag}`, key);
  }

  return data;
}

export async function invalidate(tag: string): Promise<void> {
  const keys = await redis.smembers(`tag:${tag}`);
  if (keys.length > 0) {
    await redis.del(...keys);
  }
  await redis.del(`tag:${tag}`);
}
```

**Usage in API Routes**:
```typescript
// app/api/strategies/route.ts
import { cached, invalidate } from '@/lib/cache';

export async function GET(req: Request) {
  const user = await getUser();

  const strategies = await cached(
    `strategies:${user.id}`,
    async () => {
      const { data } = await supabase
        .from('strategies')
        .select('*')
        .eq('user_id', user.id);
      return data;
    },
    { ttl: 300, tags: ['strategies', `user:${user.id}`] }
  );

  return Response.json(strategies);
}

export async function POST(req: Request) {
  const user = await getUser();
  const body = await req.json();

  // Create strategy
  const { data } = await supabase
    .from('strategies')
    .insert({ ...body, user_id: user.id });

  // Invalidate cache
  await invalidate(`user:${user.id}`);

  return Response.json(data);
}
```

**Expected Outcome**:
- API response time: 200ms â†’ 50ms (4x improvement)
- Database load: -70% (cache hit rate 60-80%)
- Cost savings: 30% (fewer DB queries)

---

#### **Phase 2: Database Functions** (Day 3-5, 15-20 hours)

**Step 1: Analyze API Routes (2ì‹œê°„)**
```bash
# Find N+1 query patterns
rg "for.*await supabase" src/app/api/
rg "map.*supabase" src/app/api/
```

**Step 2: Create Database Functions (6-8ì‹œê°„)**

**Function 1: get_strategies_with_performance**
```sql
-- supabase/migrations/20251217_create_functions.sql
CREATE OR REPLACE FUNCTION get_strategies_with_performance(p_user_id UUID)
RETURNS TABLE (
  id UUID,
  name TEXT,
  description TEXT,
  created_at TIMESTAMPTZ,
  updated_at TIMESTAMPTZ,
  avg_sharpe NUMERIC,
  avg_cagr NUMERIC,
  avg_return NUMERIC,
  backtest_count INTEGER,
  last_backtest_at TIMESTAMPTZ
)
SECURITY DEFINER
SET search_path = public
LANGUAGE plpgsql
AS $$
BEGIN
  RETURN QUERY
  SELECT
    s.id,
    s.name,
    s.description,
    s.created_at,
    s.updated_at,
    ROUND(AVG(b.sharpe_ratio), 2) as avg_sharpe,
    ROUND(AVG(b.cagr), 2) as avg_cagr,
    ROUND(AVG(b.total_return), 2) as avg_return,
    COUNT(b.id)::INTEGER as backtest_count,
    MAX(b.created_at) as last_backtest_at
  FROM strategies s
  LEFT JOIN backtest_results b ON s.id = b.strategy_id
  WHERE s.user_id = p_user_id
  GROUP BY s.id
  ORDER BY s.created_at DESC;
END;
$$;

GRANT EXECUTE ON FUNCTION get_strategies_with_performance TO authenticated;

COMMENT ON FUNCTION get_strategies_with_performance IS
  'Returns user strategies with aggregated performance metrics. Eliminates N+1 queries.';
```

**Function 2: create_backtest_job**
```sql
CREATE OR REPLACE FUNCTION create_backtest_job(
  p_user_id UUID,
  p_strategy_id UUID,
  p_config JSONB
)
RETURNS UUID
SECURITY DEFINER
SET search_path = public
LANGUAGE plpgsql
AS $$
DECLARE
  v_job_id UUID;
  v_strategy_exists BOOLEAN;
BEGIN
  -- Validation
  SELECT EXISTS (
    SELECT 1 FROM strategies
    WHERE id = p_strategy_id AND user_id = p_user_id
  ) INTO v_strategy_exists;

  IF NOT v_strategy_exists THEN
    RAISE EXCEPTION 'Strategy not found or unauthorized';
  END IF;

  -- Create job
  INSERT INTO backtest_jobs (user_id, strategy_id, config, status)
  VALUES (p_user_id, p_strategy_id, p_config, 'queued')
  RETURNING id INTO v_job_id;

  -- Notify queue worker
  PERFORM pg_notify('backtest_queue', v_job_id::TEXT);

  RETURN v_job_id;
EXCEPTION
  WHEN others THEN
    RAISE EXCEPTION 'Failed to create backtest job: %', SQLERRM;
END;
$$;

GRANT EXECUTE ON FUNCTION create_backtest_job TO authenticated;
```

**Step 3: Update API Routes (4-6ì‹œê°„)**
```typescript
// app/api/strategies/route.ts (AFTER)
export async function GET(req: Request) {
  const user = await getUser();

  // Single function call (no N+1)
  const { data: strategies, error } = await supabase
    .rpc('get_strategies_with_performance', {
      p_user_id: user.id
    });

  if (error) throw error;

  return Response.json(strategies);
}
```

**Step 4: Testing (2-3ì‹œê°„)**
```typescript
// tests/api/strategies.test.ts
describe('GET /api/strategies', () => {
  it('should return strategies with performance', async () => {
    const response = await fetch('/api/strategies', {
      headers: { Authorization: `Bearer ${token}` }
    });

    const strategies = await response.json();

    expect(strategies[0]).toHaveProperty('avg_sharpe');
    expect(strategies[0]).toHaveProperty('backtest_count');
  });

  it('should be faster than 100ms', async () => {
    const start = Date.now();
    await fetch('/api/strategies', {
      headers: { Authorization: `Bearer ${token}` }
    });
    const duration = Date.now() - start;

    expect(duration).toBeLessThan(100);
  });
});
```

**Expected Outcome**:
- API response time: 300ms â†’ 50ms (6x improvement)
- Code complexity: Reduced (logic in DB)
- N+1 queries: Eliminated

---

### Week 3-4: Automation & Features (Jan 1-14)

#### **Phase 4: Database Triggers** (4-5 days)

**Use Case 1: Audit Logging**
```sql
-- supabase/migrations/20251218_create_triggers.sql
CREATE TABLE audit_logs (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  table_name TEXT NOT NULL,
  record_id UUID NOT NULL,
  action TEXT NOT NULL,
  old_data JSONB,
  new_data JSONB,
  user_id UUID,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE OR REPLACE FUNCTION audit_trigger_func()
RETURNS TRIGGER AS $$
BEGIN
  IF TG_OP = 'DELETE' THEN
    INSERT INTO audit_logs (table_name, record_id, action, old_data, user_id)
    VALUES (TG_TABLE_NAME, OLD.id, TG_OP, to_jsonb(OLD), OLD.user_id);
    RETURN OLD;
  ELSIF TG_OP = 'UPDATE' THEN
    INSERT INTO audit_logs (table_name, record_id, action, old_data, new_data, user_id)
    VALUES (TG_TABLE_NAME, NEW.id, TG_OP, to_jsonb(OLD), to_jsonb(NEW), NEW.user_id);
    RETURN NEW;
  ELSIF TG_OP = 'INSERT' THEN
    INSERT INTO audit_logs (table_name, record_id, action, new_data, user_id)
    VALUES (TG_TABLE_NAME, NEW.id, TG_OP, to_jsonb(NEW), NEW.user_id);
    RETURN NEW;
  END IF;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER strategies_audit
AFTER INSERT OR UPDATE OR DELETE ON strategies
FOR EACH ROW EXECUTE FUNCTION audit_trigger_func();
```

**Use Case 2: Auto-Refresh Materialized Views**
```sql
CREATE OR REPLACE FUNCTION refresh_perf_on_backtest()
RETURNS TRIGGER AS $$
BEGIN
  IF NEW.status = 'completed' AND OLD.status != 'completed' THEN
    REFRESH MATERIALIZED VIEW CONCURRENTLY strategy_performance_agg;
  END IF;
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER backtest_completion_refresh
AFTER UPDATE ON backtest_jobs
FOR EACH ROW
EXECUTE FUNCTION refresh_perf_on_backtest();
```

---

#### **Phase 7: Scheduled Tasks** (2 days)

**Enable pg_cron**:
```sql
CREATE EXTENSION IF NOT EXISTS pg_cron;

-- Daily materialized view refresh (2 AM UTC)
SELECT cron.schedule(
  'refresh-strategy-performance',
  '0 2 * * *',
  $$REFRESH MATERIALIZED VIEW CONCURRENTLY strategy_performance_agg$$
);

-- Weekly digest (Monday 9 AM)
SELECT cron.schedule(
  'weekly-digest',
  '0 9 * * 1',
  $$
    INSERT INTO notification_queue (user_id, type, payload)
    SELECT
      user_id,
      'weekly_digest',
      jsonb_build_object(
        'week', date_trunc('week', NOW()),
        'total_return', SUM(total_return)
      )
    FROM strategies s
    JOIN backtest_results b ON s.id = b.strategy_id
    WHERE b.created_at >= NOW() - INTERVAL '7 days'
    GROUP BY user_id;
  $$
);

-- Cleanup old exports (daily 3 AM)
SELECT cron.schedule(
  'cleanup-old-exports',
  '0 3 * * *',
  $$
    DELETE FROM storage.objects
    WHERE bucket_id = 'exports'
    AND created_at < NOW() - INTERVAL '7 days';
  $$
);
```

---

### Post-Beta: Features (Jan 15+)

#### **Phase 6: Storage Integration** (3-4 days)

**Create Buckets & RLS**:
```sql
-- Create buckets in Supabase Dashboard â†’ Storage
-- backtest-charts (private)
-- exports (private)
-- templates (public)

-- RLS policies
CREATE POLICY "Users can access own backtest charts"
ON storage.objects FOR SELECT
USING (
  bucket_id = 'backtest-charts' AND
  (storage.foldername(name))[1] IN (
    SELECT id::TEXT FROM backtest_results WHERE user_id = auth.uid()
  )
);

CREATE POLICY "Templates are public"
ON storage.objects FOR SELECT
USING (bucket_id = 'templates');
```

**Save Charts to Storage**:
```typescript
// lib/storage/saveBacktestChart.ts
export async function saveBacktestChart(
  backtestId: string,
  chartData: any
): Promise<string> {
  // Generate chart image
  const canvas = generateChart(chartData);
  const blob = await canvas.toBlob('image/png');

  // Upload to Storage
  const { data, error } = await supabase.storage
    .from('backtest-charts')
    .upload(`${backtestId}/equity-curve.png`, blob, {
      cacheControl: '3600',
      upsert: true
    });

  if (error) throw error;

  // Get public URL
  const { data: { publicUrl } } = supabase.storage
    .from('backtest-charts')
    .getPublicUrl(data.path);

  // Save URL to database
  await supabase
    .from('backtest_results')
    .update({ chart_url: publicUrl })
    .eq('id', backtestId);

  return publicUrl;
}
```

---

## ğŸ“ˆ Success Metrics

### Performance Metrics
| Metric | Baseline | Target | Phase |
|--------|----------|--------|-------|
| API P95 Latency | 300ms | < 100ms | Phase 2 |
| Dashboard Load Time | 10s | < 2s | Phase 1 |
| Database Queries/sec | 1000 | < 300 | Phase 5 |
| Cache Hit Rate | 0% | > 70% | Phase 5 |
| Error Rate | < 5% | < 1% | All |

### Scalability Metrics
| Metric | Baseline | Target | Phase |
|--------|----------|--------|-------|
| Max Concurrent Users | 100 | 10,000 | Phase 3 |
| Connection Pool Usage | N/A | < 80% | Phase 3 |
| Realtime Reconnection | N/A | < 5% | Phase 1 |

### Business Metrics
| Metric | Baseline | Target | Phase |
|--------|----------|--------|-------|
| User Retention (D7) | TBD | > 40% | Phase 1 |
| Avg Session Duration | TBD | > 10min | Phase 1 |
| Strategies Created/User | TBD | > 3 | All |

---

## ğŸš¨ Risk Mitigation

### Critical Risks

**Risk 1: Realtime Connection Instability**
- **Mitigation**: Auto-reconnection with exponential backoff
- **Fallback**: Polling if reconnection fails after 5 attempts
- **Monitoring**: Track reconnection rate (< 5% target)

**Risk 2: Database Function Breaking Changes**
- **Mitigation**: Blue-green deployment (v1 + v2 coexist)
- **Rollback**: Keep old functions until new ones proven
- **Testing**: A/B test with 10% traffic first

**Risk 3: Cache Invalidation Bugs**
- **Mitigation**: Short TTL (5 minutes max)
- **Fallback**: Database trigger invalidation
- **Monitoring**: Track cache hit rate anomalies

**Risk 4: Connection Pool Exhaustion**
- **Mitigation**: Rate limiting (100 req/min per user)
- **Monitoring**: Alert if pool usage > 80%
- **Scaling**: Upgrade Supabase tier if needed

---

## ğŸ’° Cost Analysis

### Current (Beta, Free Tier)
- Supabase: $0/month
- Upstash Redis: $0/month
- Vercel: $0/month
- **Total: $0/month**

### Projected (1,000 Active Users, Pro Tier)
- Supabase Pro: $25/month
- Upstash Redis Pro: $10/month
- Vercel Pro: $20/month
- **Total: $55/month** ($0.055 per user)

### ROI
- **Investment**: $5,000 (14-18 days development)
- **Monthly Savings**: $300 (infrastructure + operational)
- **Payback Period**: 17 months
- **3-Year Net Profit**: $5,800

---

## ğŸ“š Documentation

### Files to Create
- [x] `SUPABASE_MASTER_PLAN.md` (this file)
- [ ] `PHASE_1_REALTIME.md` (detailed checklist)
- [ ] `PHASE_2_FUNCTIONS.md` (detailed checklist)
- [ ] `PHASE_3_POOLING.md` (detailed checklist)
- [ ] `PHASE_4_TRIGGERS.md` (detailed checklist)
- [ ] `PHASE_5_CACHE.md` (detailed checklist)
- [ ] `PHASE_6_STORAGE.md` (detailed checklist)
- [ ] `PHASE_7_CRON.md` (detailed checklist)
- [ ] `MONITORING_DASHBOARD.md` (setup guide)
- [ ] `RUNBOOK.md` (emergency procedures)

---

## âœ… Immediate Next Steps (TODAY)

### Step 1: Complete Beta Launch
- [ ] Vercel ë¡œê·¸ì¸ â†’ Redeploy í´ë¦­
- [ ] Production URL í…ŒìŠ¤íŠ¸
- [ ] Beta ì´ˆëŒ€ ì´ë©”ì¼ ë°œì†¡ (ì²« 20ëª…)
- [ ] Discord ì„œë²„ ì˜¤í”ˆ

### Step 2: Prepare for Week 1 (Tomorrow Morning)
- [ ] Read `PHASE_3_POOLING.md`
- [ ] Supabase Dashboard ì ‘ì† ì¤€ë¹„
- [ ] Connection Pooling í™œì„±í™” (1 hour)
- [ ] Monitor deployment for issues

### Step 3: Start Phase 1 (Day 2-3)
- [ ] Read `PHASE_1_REALTIME.md`
- [ ] Enable Realtime on tables
- [ ] Create realtime hooks
- [ ] Integrate into Dashboard
- [ ] Test with 2 browser tabs

---

## ğŸ‰ Expected Final Result

**After All 7 Phases:**
- âœ… Dashboard: Real-time, Responsive, Professional (100/100)
- âœ… Backend: Fast, Scalable, Maintainable (95/100)
- âœ… Infrastructure: Robust, Monitored, Automated (95/100)
- âœ… Overall: Enterprise-grade (97/100)

**Performance:**
- API: 10x faster (300ms â†’ 30ms)
- Page load: 5x faster (10s â†’ 2s)
- Scalability: 100x capacity (100 â†’ 10,000 users)

**User Experience:**
- Real-time updates (no refresh)
- Toast notifications (immediate feedback)
- Professional polish (Robinhood-level)

**Operational Efficiency:**
- Automated tasks (pg_cron)
- Audit logging (compliance)
- Monitoring dashboard (observability)

---

**Ready to Launch! ğŸš€**

Last Updated: 2025-12-17
Next Review: Beta Week 1 (2025-12-24)
